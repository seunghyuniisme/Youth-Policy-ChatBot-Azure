from flask import Flask, request, jsonify, send_file, render_template, send_from_directory
from flask_cors import CORS
from openai import AzureOpenAI
import requests
import os
import logging
app = Flask(__name__)
CORS(app)
speaker_profile_id = None
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Azure OpenAI ì„¤ì •
azure_oai_endpoint = 'https://hani-openai.openai.azure.com/'
azure_oai_key = '7a810a8e3cae4362810d8bb19f8e9324'
azure_oai_deployment = 'gpt-4o'

# Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = AzureOpenAI(
    azure_endpoint=azure_oai_endpoint,
    api_key=azure_oai_key,
    api_version="2024-02-15-preview"
)

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë¡œë“œ í•¨ìˆ˜
def load_system_message(character):
    file_name = f"system_message_{character}.txt"
    file_path = os.path.join('prompts', file_name)
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            system_message = file.read()
            print(f"Loaded system message for {character}: {system_message}")
            return system_message
    except FileNotFoundError:
        print(f"System message file not found for character: {character}")
        return "ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€"


# ë„¤ì•„ë²„ ë¸”ë¡œê·¸ API 
def search_naver_blog(query):
    client_id = 'vTwwnYEG531P5MTPVUcv'
    client_secret = 'zpC7PzW6ms'
    url = "https://openapi.naver.com/v1/search/blog"
    headers = {
        'X-Naver-Client-Id': client_id,
        'X-Naver-Client-Secret': client_secret
    }
    params = {
        'query': query,
        'display': 5,
        'start': 1
    }

    response = requests.get(url, headers=headers, params=params)
    
    if response.status_code == 200:
        data = response.json()
        results = []
        for index, item in enumerate(data['items'], start=1):
            # HTML í˜•ì‹ìœ¼ë¡œ ë§í¬ ìƒì„±
            link = f"{index}. <a href='{item['link']}' target='_blank'>{item['title']}</a>"
            results.append(link)
        # HTML í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜
        return "<br>".join(results)
    else:
        return f"Error with Naver API: {response.status_code}"




# ëŒ€í™” ë‚´ì—­ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ì „ì—­ ë³€ìˆ˜
message_array = []

# Set up logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# ê¸°ë³¸ ë¼ìš°íŠ¸
@app.route('/')
def index():
    return render_template('main.html')

# select.html ë¼ìš°íŠ¸ ì¶”ê°€
@app.route('/select.html')
def select():
    return render_template('select.html')

# index.html ê²½ë¡œë¥¼ ì²˜ë¦¬í•˜ëŠ” ë¼ìš°íŠ¸
@app.route('/index.html')
def index_html():
    return render_template('index.html')

# ì •ì  íŒŒì¼ ê²½ë¡œ ì œê³µ
@app.route('/static/<path:path>')
def send_static(path):
    return send_from_directory('static', path)

# ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ì œê³µ
@app.route('/image/<path:filename>')
def send_image(filename):
    return send_from_directory('static/image', filename)





#ì±„íŒ…
@app.route('/chat', methods=['POST'])
def chat():
    try:
        user_input = request.json.get('prompt')
        selected_character = request.json.get('character')

        if not user_input or not selected_character:
            return jsonify({"error": "No prompt or character provided"}), 400

        # ìƒˆë¡œìš´ ìºë¦­í„° ì„ íƒ ì‹œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ë˜, ëŒ€í™” ë‚´ì—­ì€ ìœ ì§€
        system_message = load_system_message(selected_character)
        if message_array and message_array[0]["role"] == "system":
            message_array[0]["content"] = system_message  # ê¸°ì¡´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ êµì²´
        else:
            message_array.insert(0, {"role": "system", "content": system_message})  # ìƒˆë¡œìš´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€

        # ë¦¬ë·° ê´€ë ¨ ì…ë ¥ ì²˜ë¦¬
        if 'ë¦¬ë·°' in user_input:
            review_index = user_input.find('ë¦¬ë·°')
            search_query = user_input[:review_index].strip()

            if search_query:
                search_query += ' ì²­ë…„'
            else:
                search_query = 'ì²­ë…„ ë¦¬ë·°'

            naver_search_result = search_naver_blog(search_query)

            review_guide = ("ì•„ë˜ëŠ” ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ğŸ” ê¸°ë°˜ì˜ ë¦¬ë·° ì •ë³´ì…ë‹ˆë‹¤.\n"
                            "ë§í¬ë¥¼ í´ë¦­ğŸ‘†í•˜ë©´ ê´€ë ¨ ë¦¬ë·°ë¥¼ í™•ì¸ğŸ‘€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n")
            full_response = review_guide + naver_search_result

            # ë¦¬ë·° ê²°ê³¼ë¥¼ ì‚¬ìš©ìì—ê²Œ ë°”ë¡œ ì „ì†¡
            return jsonify({"response": full_response})

        # Add user input to the message array
        message_array.append({"role": "user", "content": user_input})

        # Generate response from Azure OpenAI
        response = client.chat.completions.create(
            model=azure_oai_deployment,
            temperature=0.7,
            max_tokens=1200,
            messages=message_array
        )

        generated_text = response.choices[0].message.content

        # Add the AI response to the message array to keep the conversation going
        message_array.append({"role": "assistant", "content": generated_text})

        # Return the generated response as JSON
        return jsonify({"response": generated_text})

    except Exception as ex:
        # Check if the error is related to content filtering
        if 'content_filter_result' in str(ex):
            return jsonify({"error": "The content was filtered due to Azure OpenAI's content management policy."}), 400
        else:
            print(f"Exception occurred: {ex}")
            return jsonify({"error": str(ex)}), 500
        

        
#ê°€ì¥ìµœê·¼íŒŒì¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
@app.route('/latest-audio-file', methods=['GET'])
def latest_audio_file():
    try:
        file_name = get_latest_audio_file()
        if file_name:
            file_path = os.path.join('static', 'audio', file_name)
            if os.path.exists(file_path):
                logger.info(f"Streaming the latest audio file: {file_name}")
                return send_file(file_path, as_attachment=False)
            else:
                logger.error(f"Audio file not found: {file_name}")
                return jsonify({"error": "Audio file not found"}), 404
        else:
            return jsonify({"error": "No audio files found or an error occurred."}), 404
    except Exception as e:
        logger.error(f"Error processing request for latest audio file: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

# í…ìŠ¤íŠ¸ ì˜¤ë””ì˜¤ ê¸°ëŠ¥ì „í™˜
@app.route('/audio/convert', methods=['POST'])
def convert_audio():
    try:
        # ìµœì‹  í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        if not message_array or message_array[-1]["role"] != "assistant":
            return jsonify({"error": "No latest assistant message found"}), 400
        
        latest_text = message_array[-1]["content"]
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„±
        audio_file_path = generate_audio(latest_text)
        if audio_file_path:
            # send_file í•¨ìˆ˜ì—ì„œ attachment_filenameì„ download_nameìœ¼ë¡œ ë³€ê²½
            return send_file(audio_file_path, mimetype='audio/wav', as_attachment=True, download_name='generated_audio.wav')
        else:
            return jsonify({"error": "Failed to generate audio"}), 500

    except Exception as ex:
        print(f"Error in convert_audio: {ex}")
        return jsonify({"error": str(ex)}), 500

# í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì˜¤ë””ì˜¤ íŒŒì¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def generate_audio(text: str) -> str:
    global speaker_profile_id  # ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©
    try:
        if not speaker_profile_id:
            raise ValueError("Speaker profile ID is not set. Please create a personal voice first.")
        # Log speakerProfileId
        print(f'Using speaker profile ID: {speaker_profile_id}')
        
        # í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ íŒŒì¼ ì €ì¥
        output_file_path = request_tts(text, speaker_profile_id)
        if output_file_path:
            return output_file_path
        else:
            raise Exception("Failed to generate audio using Azure TTS API.")
    except Exception as e:
        print(f"Error generating audio: {e}")
        return None


#ê°€ì¥ ìµœê·¼ ì˜¤ë””ì˜¤íŒŒì¼ ë¶ˆëŸ¬ì˜´
def get_latest_audio_file(directory='static/audio/'):
    try:
        # 'static/audio/' ë””ë ‰í† ë¦¬ì—ì„œ .wav í™•ì¥ìë¥¼ ê°€ì§„ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        audio_files = [f for f in os.listdir(directory) if f.endswith('.wav')]
        if not audio_files:
            logger.error("No audio files found in the specified directory.")
            return None

        # íŒŒì¼ì„ ìƒì„± ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  ê°€ì¥ ìµœê·¼ íŒŒì¼ì„ ì„ íƒí•©ë‹ˆë‹¤.
        latest_file = max(audio_files, key=lambda f: os.path.getctime(os.path.join(directory, f)))
        
        logger.info(f"Latest audio file found: {latest_file}")
        return latest_file
    except Exception as e:
        logger.error(f"Error getting latest audio file: {e}", exc_info=True)
        return None















##########################ì—¬ê¸°ì„œë¶€í„° ìŒì„±#######################################
#!/usr/bin/env python
# coding: utf-8

# Copyright (c) Microsoft. All rights reserved.
# Licensed under the MIT license. See LICENSE.md file in the project root for full license information.

import json
import requests
from time import sleep
import os
import logging
try:
    import customvoice
except ImportError:
    print('Pleae copy folder https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/samples/custom-voice/python/customvoice and keep the same folder structure as github.' )
    quit()
import azure.cognitiveservices.speech as speechsdk

def speech_synthesis_to_wave_file(text: str, output_file_path: str, speaker_profile_id: str = None):
    # Creates an instance of a speech config with specified subscription key and service region.
    speech_config = speechsdk.SpeechConfig(subscription=config.key, region=config.region)
    speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm)
    file_config = speechsdk.audio.AudioOutputConfig(filename=output_file_path)
    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=file_config)

    if speaker_profile_id:
        # speaker_profile_idê°€ ìˆëŠ” ê²½ìš° í•´ë‹¹ IDë¥¼ ì‚¬ìš©í•˜ëŠ” SSML ìƒì„±
        ssml = (
            "<speak version='1.0' xml:lang='ko-KR' xmlns='http://www.w3.org/2001/10/synthesis' "
            "xmlns:mstts='http://www.w3.org/2001/mstts'>"
            "<voice name='DragonLatestNeural'>"
            f"<mstts:ttsembedding speakerProfileId='{speaker_profile_id}'/>"
            "<mstts:express-as style='Prompt'>"
            f"<lang xml:lang='ko-KR'>{text}</lang>"
            "</mstts:express-as>"
            "</voice></speak>"
        )
    else:
        # speaker_profile_idê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ëª©ì†Œë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” SSML ìƒì„±
        ssml = (
            "<speak version='1.0' xml:lang='ko-KR'>"
            "<voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-JiMinNeural'>"
            f"{text}"
            "</voice></speak>"
        )

    def word_boundary(evt):
        print(f"Word Boundary: Text='{evt.text}', Audio offset={evt.audio_offset / 10000}ms, Duration={evt.duration / 10000}ms, text={evt.text}")

    speech_synthesizer.synthesis_word_boundary.connect(word_boundary)
    result = speech_synthesizer.speak_ssml_async(ssml).get()

    # Check result
    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        print(f"Speech synthesized for text [{text}], and the audio was saved to [{output_file_path}]")
        print(f"result id: {result.result_id}")
    elif result.reason == speechsdk.ResultReason.Canceled:
        cancellation_details = result.cancellation_details
        print(f"Speech synthesis canceled: {cancellation_details.reason}")
        if cancellation_details.reason == speechsdk.CancellationReason.Error:
            print(f"Error details: {cancellation_details.error_details}")
        print(f"result id: {result.result_id}")
#ìœ íš¨ì„œê²€ì‚¬
def retrieve_existing_speaker_profile_id(project_id: str, personal_voice_id: str) -> str:
    """
    ì´ í•¨ìˆ˜ëŠ” ê¸°ì¡´ì˜ personal voice IDë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ speaker profile IDë¥¼ ê²€ìƒ‰í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # ì—¬ê¸°ì— ì‹¤ì œë¡œ ê¸°ì¡´ speaker profile IDë¥¼ ê²€ìƒ‰í•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•˜ì‹­ì‹œì˜¤.
        # ì˜ˆì‹œë¡œ, í”„ë¡œì íŠ¸ì˜ ìƒíƒœë¥¼ ì¡°íšŒí•˜ê±°ë‚˜ íŠ¹ì • APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # ì•„ë˜ì˜ ì½”ë“œëŠ” ì˜ˆì‹œì´ë©° ì‹¤ì œ êµ¬í˜„ì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
        response = customvoice.PersonalVoice.get(config, personal_voice_id)
        if response and response.speaker_profile_id:
            return response.speaker_profile_id
        else:
            print("No existing speaker profile ID found.")
            return None
    except Exception as e:
        print(f"Error retrieving existing speaker profile ID: {e}")
        return None

#speaker_profile_idìƒì„±ì„ ë„ì™€ì£¼ëŠ” í•¨ìˆ˜
def create_personal_voice(project_id: str,
                          consent_id: str, consent_file_path: str, voice_talent_name: str, company_name: str,
                          personal_voice_id: str, audio_folder: str):
    try:
        # Create project
        project = customvoice.Project.create(config, project_id, customvoice.ProjectKind.PersonalVoice)
        print(f'Project created. project id: {project.id}')

        # Upload consent
        consent = customvoice.Consent.create(config, project_id, consent_id, voice_talent_name, company_name, consent_file_path, 'ko-KR')
        if consent.status == customvoice.Status.Failed:
            print(f'Create consent failed. consent id: {consent.id}')
            raise Exception
        elif consent.status == customvoice.Status.Succeeded:
            print(f'Create consent succeeded. consent id: {consent.id}')

        # Create personal voice
        personal_voice = customvoice.PersonalVoice.create(config, project_id, personal_voice_id, consent_id, audio_folder)
        if personal_voice.status == customvoice.Status.Failed:
            print(f'Create personal voice failed. personal voice id: {personal_voice.id}')
            raise Exception
        elif personal_voice.status == customvoice.Status.Succeeded:
            print(f'Create personal voice succeeded. personal voice id: {personal_voice.id}, speaker profile id: {personal_voice.speaker_profile_id}')

        # Check and log speaker profile ID
        if personal_voice.speaker_profile_id:
            print(f'Speaker Profile ID: {personal_voice.speaker_profile_id}')
        else:
            print('Speaker Profile ID not found.')
            raise ValueError("Speaker Profile ID was not created properly.")

        return personal_voice.speaker_profile_id

    except Exception as e:
        if "Resource Id already exists" in str(e):
            print("Resource already exists. Retrieving existing speaker profile ID...")
            # Implement logic to retrieve the existing speaker profile ID
            existing_speaker_profile_id = retrieve_existing_speaker_profile_id(project_id, personal_voice_id)
            if existing_speaker_profile_id:
                print(f'Retrieved existing speaker profile ID: {existing_speaker_profile_id}')
                return existing_speaker_profile_id
            else:
                raise ValueError("Failed to retrieve existing speaker profile ID.")
        else:
            raise e




def clean_up(project_id: str, consent_id: str, personal_voice_id: str):
    customvoice.PersonalVoice.delete(config, personal_voice_id)
    customvoice.Consent.delete(config, consent_id)
    customvoice.Project.delete(config, project_id)


region = 'eastus' # eastus, westeurope, southeastasia
key = '303e8feb39e949be9df0929bc0c93390'


logging.basicConfig(filename="customvoice.log",
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                    filemode='w')
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

config = customvoice.Config(key, region, logger)


# project_id = 'personal-voice-project-1'
# consent_id = 'personal-voice-consent-1'
# personal_voice_id  = 'personal-voice-1'
project_id = 'test1'
consent_id = 'test1'
personal_voice_id  = 'test-speaker'

try:
    # step 1: create personal voice
    # Need consent file and audio file to create personal vocie.
    # This is consent file template.
    # I [voice talent name] am aware that recordings of my voice will be used by [company name] to create and use a synthetic version of my voice.
    # You can find sample consent file here
    # https://github.com/Azure-Samples/Cognitive-Speech-TTS/blob/master/CustomVoice/Sample%20Data/Individual%20utterances%20%2B%20matching%20script/VoiceTalentVerbalStatement.wav
    consent_file_path = r'TestData/agreement.wav'
    voice_talent_name = 'ì„œì˜ˆê±´'
    company_name = 'í•˜ë‹ˆë°”ëŒ'

    # Need 5 - 90 seconds audio file.
    # You can find sample audio file here.
    # https://github.com/Azure-Samples/Cognitive-Speech-TTS/blob/master/CustomVoice/Sample%20Data/Individual%20utterances%20%2B%20matching%20script/SampleAudios.zip
    audio_folder = r'TestData/voice/'
    speaker_profile_id = create_personal_voice(project_id, 
                                            consent_id, consent_file_path, voice_talent_name, company_name,
                                            personal_voice_id, audio_folder)

    # step 2: synthesis wave
    text = 'ì•ˆë…•í•˜ì„¸ìš”? ìƒì¾Œí•œ ìˆ˜ìš”ì¼ ì•„ì¹¨ì…ë‹ˆë‹¤.'
    output_wave_file_path = 'output_sdk.wav'
    speech_synthesis_to_wave_file(text, output_wave_file_path, speaker_profile_id)
except Exception as e:
    print(e)
    
    

# finally:
#     # Optional step 3: clean up, if you don't need this voice to synthesis more content.
#     clean_up(project_id, consent_id, personal_voice_id)
###############################################################ai speechê°€ì ¸ì˜¤ëŠ”ë¶€ë¶„ í•¨ìˆ˜########################
import requests

# Azure Speech API ì¸ì¦ ì •ë³´
subscription_key = "303e8feb39e949be9df0929bc0c93390"
endpoint_id = "b134f8d5-2716-4f2e-abfe-62b1705deaf9"
region = "eastus"


# Azure Speech API í† í°ì„ ì–»ëŠ” í•¨ìˆ˜
def get_token():
    endpoint = f"https://{region}.api.cognitive.microsoft.com/sts/v1.0/issueToken"
    headers = {
        "Ocp-Apim-Subscription-Key": subscription_key,
    }

    response = requests.post(endpoint, headers=headers)

    if response.status_code == 200:
        return response.text
    else:
        return ''

def request_tts(text, speaker_profile_id):
    endpoint = f"https://{region}.tts.speech.microsoft.com/cognitiveservices/v1"
    access_token = get_token()

    if not access_token:
        print("Failed to obtain access token.")
        return None

    headers = {
        "Content-Type": "application/ssml+xml",
        "X-Microsoft-OutputFormat": "riff-24khz-16bit-mono-pcm",
        "Authorization": f"Bearer {access_token}"
    }

    # SSML (Speech Synthesis Markup Language) í¬ë§·
    ssml = (
        "<speak version='1.0' xml:lang='ko-KR' xmlns='http://www.w3.org/2001/10/synthesis' "
        "xmlns:mstts='http://www.w3.org/2001/mstts'>"
        f"<voice name='DragonLatestNeural'>"
        f"<mstts:ttsembedding speakerProfileId='{speaker_profile_id}'/>"
        "<mstts:express-as style='Prompt'>"
        f"<lang xml:lang='ko-KR'>{text}</lang>"
        "</mstts:express-as>"
        "</voice></speak>"
    )

    # ë°ì´í„°ë¥¼ UTF-8ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
    response = requests.post(endpoint, headers=headers, data=ssml.encode('utf-8'))

    if response.status_code == 200:
        file_name = 'static/audio/generated_audio.wav'
        with open(file_name, "wb") as audio_file:
            audio_file.write(response.content)
        return file_name
    else:
        print(f"Error during TTS request: {response.status_code}, {response.text}")
        return None


if __name__ == '__main__':
    if speaker_profile_id is None:
        speaker_profile_id = create_personal_voice(project_id, consent_id, consent_file_path, voice_talent_name, company_name, personal_voice_id, audio_folder)
    
    app.run(debug=True)
